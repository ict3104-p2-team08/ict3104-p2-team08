{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ict3104-p2-team08/ict3104-p2-team08/blob/inference_feat/3104_T08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yJLCy-Wv_Rd"
   },
   "source": [
    "# Activity Detection Notebook\n",
    "Welcome to the ICT3104 Team08 activity Detection notebook! You'll be taken through the steps of running a pretrained activity detection model on videos and use your own trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyKsO3crw1K5"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "This section provides the setting up of the environment and dependencies required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heoGQJ7NyBiM",
    "outputId": "a2fa6fd7-2874-4e82-9914-1d335a7735cd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch -f https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install torchaudio -f https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install torchsummary==1.5.1 torchtext==0.13.1 torchvision -f https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FQXemWrcw4pE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown, widgets\n",
    "import os\n",
    "from IPython.display import HTML, Video, clear_output\n",
    "from base64 import b64encode\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Wandb\n",
    "Create an account and Login with Wandb to view analytical charts of the activity detection using the following link: https://wandb.ai/site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cee34df1c74bfb9117ed3a99cf4e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Username:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fa8622ebd34c4f86a8ef635a620c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password(description='Api key:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_username = widgets.Text(description='Username:')\n",
    "display(wandb_username)\n",
    "\n",
    "wandb_key = widgets.Password(\n",
    "    description='Api key:',\n",
    "    disabled=False)\n",
    "display(wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a49875831cf4f45ab0b1a5087f6a099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Login', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b709089cb84f2abc5454226944ec8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\smxli/.netrc\n"
     ]
    }
   ],
   "source": [
    "button = widgets.Button(description=\"Login\")\n",
    "\n",
    "display(button)\n",
    "\n",
    "def on_button_click(b):\n",
    "    !wandb login {wandb_key.value}\n",
    "\n",
    "button.on_click(on_button_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcEvKDf2wVL2"
   },
   "source": [
    "## Toggling of pipelines\n",
    "\n",
    "Here we will choose which feature extraction pipeline dependencies we will use between TSU, STEP and MCTCT.\n",
    "The selected pipeline will be loaded authomatically whereby changes can be made at any moment in the next cell and normal execution of the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lb4Grr9wuhWu",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440543ffa0a748789c10d50af8e1ddc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Pipelines:', options=('TSU', 'STEP', 'MSTCT'), value='TSU')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_pipeline = \"TSU\"\n",
    "rb = widgets.RadioButtons(\n",
    "    options=['TSU', 'STEP', 'MSTCT'],\n",
    "    description='Pipelines:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    global current_pipeline\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        if current_pipeline != change['new']:\n",
    "            if change['new'] == \"TSU\":\n",
    "                %cd -q ../\n",
    "            elif change['new'] == \"STEP\":\n",
    "                if os.path.basename(os.path.normpath(os.getcwd())) ==\"MSTCT\":\n",
    "                    %cd -q ..\n",
    "                    %cd -q  ./STEP\n",
    "                else:\n",
    "                    %cd -q  ./STEP\n",
    "            elif change['new'] == \"MSTCT\":\n",
    "                if os.path.basename(os.path.normpath(os.getcwd())) ==\"STEP\":\n",
    "                    %cd -q ../\n",
    "                    %cd -q ./MSTCT\n",
    "                else:\n",
    "                    %cd -q ./MSTCT\n",
    "        current_pipeline = change['new']\n",
    "        print(\"Current selected pipeline: {:<6}\".format(current_pipeline), end='\\r')\n",
    "\n",
    "rb.observe(on_change)\n",
    "\n",
    "display(rb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importing Datasets\n",
    "More videos from the followiing link can be added using the steps below in the respective folders: https://github.com/cvdfoundation/ava-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TSU (RGB variant only)\n",
    "\n",
    "#### Importing TSU provided Video/CSV files.\n",
    "\n",
    "##### For video files\n",
    "Step 1. Go to /ict3104-p2-team08/data/input_files/ directory\n",
    "<br>\n",
    "Step 2. Add in video files provided by TSU (Feature extraction/inference needed)\n",
    "##### For CSV files\n",
    "Step 1. Go to /ict3104-p2-team08/data/input_csv/ directory\n",
    "<br>\n",
    "Step 2. Add in CSV files provided by TSU (Inference needed to show actual annotated result)\n",
    "\n",
    "#### Importing TSU provided NPY files.\n",
    "Step 1. Go to /Toyota_Smarthome/pipline/data/RGB_i3d_16frames_64000_SSD/ directory\n",
    "<br>\n",
    "Step 2. Add in all npy files provided by TSU\n",
    "\n",
    "#### Importing V-iashin provided RGB NPY files.\n",
    "If you have already extracted the RGB NPY files do the following:\n",
    "<br>\n",
    "Step 1. Go to /ict3104-p2-team08/Toyota_Smarthome/pipline/data/RGB_v_iashin/ directory\n",
    "<br>\n",
    "Step 2. Add in the v-iashin extracted rgb npy files\n",
    "<br>\n",
    "If not extracted can make use of feature extraction section below (Note: feature extraction is only possible for TSU videos that can be found in <br> /ict3104-p2-team08/data/input_files/ directory)\n",
    "\n",
    "#### Importing TSU provided Pretrained model (PDAN).\n",
    "Step 1. Go to /ict3104-p2-team08/Toyota_Smarthome/pipline/models/ directory\n",
    "<br>\n",
    "Step 2. Add in PDAN provided by TSU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### STEP\n",
    "\n",
    "#### Importing STEP provided Video files.\n",
    "\n",
    "##### For video files\n",
    "##### Training video files\n",
    "Step 1. Go to /ict3104-p2-team08/STEP/datasets/ava/videos/train_vid/ directory\n",
    "<br>\n",
    "Step 2. Add in training video files provided by AVA\n",
    "##### Testing video files\n",
    "Step 1. Go to /ict3104-p2-team08/STEP/datasets/ava/videos/val_vid/ directory\n",
    "<br>\n",
    "Step 2. Add in validation video files provided by AVA\n",
    "\n",
    "#### Importing STEP provided Pretrained model (ava_step.pth).\n",
    "Step 1. Go to /ict3104-p2-team08/STEP/pretrained/ directory\n",
    "<br>\n",
    "Step 2. Add in ava_step.pth provided by AVA\n",
    "\n",
    "#### To extract video frames for newly added videos (for training/testing) in /ict3104-p2-team08/STEP/datasets/ava/videos/ directory, run the below cell (Note: Run section 2 Toggling of pipelines and check STEP radio button first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -i ./scripts/extract_clips.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voFOEyiP0WSt"
   },
   "source": [
    "## Data exploration\n",
    "\n",
    "This section load and display video data from the Toyota Smarthome (TSU) project\n",
    "\n",
    "### Jupyter -- Upload any files/datasets into the main folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83178d9469af444498932d0d1cac3d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.mp4', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploader = widgets.FileUpload(accept='.mp4',  multiple=True)\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd Documents/GitHub/ict3104-p2-team08\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = uploader.value\n",
    "import pandas as pd\n",
    "\n",
    "#depends on pipeline\n",
    "\n",
    "def return_name():\n",
    "    for name, file_info in uploader.value.items():\n",
    "        name_file = name\n",
    "    return name_file\n",
    "    \n",
    "    \n",
    "def catch_err_before_begin():\n",
    "    uploader.value\n",
    "    if len(uploader.value) == 0:\n",
    "        print(\"An exception occurred\")\n",
    "        print(\"possible reason - file too large to upload. consider manual uploading of dataset in jupyter notebook\")\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "    \n",
    "def upload_to_path(dir):\n",
    "    try:\n",
    "        for name, file_info in uploader.value.items():\n",
    "            with open (name, 'wb') as file:\n",
    "                file.write(file_info['content'])\n",
    "        print(\"successful upload of\", name, \"to\", dir)\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "        \n",
    "        \n",
    "        \n",
    "# uplaod videos to data/input_files for tsu.\n",
    "# return back to root dir\n",
    "if current_pipeline == \"TSU\":\n",
    "    if catch_err_before_begin() == True:\n",
    "        %cd -q  ./data/input_files\n",
    "        upload_to_path(\"./data/input_files\")\n",
    "        %cd -q ../\n",
    "        %cd -q ../\n",
    "    \n",
    "\n",
    "# uplaod videos to data/input_files for mstct.\n",
    "# return back to mstct dir\n",
    "if current_pipeline == \"MSTCT\":\n",
    "    if catch_err_before_begin() == True:\n",
    "        %cd -q ../\n",
    "        %cd -q  ./data/input_files\n",
    "        upload_to_path(\"./data/input_files\")\n",
    "        %cd -q ../\n",
    "        %cd -q ../\n",
    "        %cd -q ./MSTCT\n",
    "    \n",
    "            \n",
    "if current_pipeline == \"STEP\":\n",
    "    #check if trainval or test\n",
    "    xl = pd.ExcelFile(\"./datasets/ava/check.xlsx\")\n",
    "    df = xl.parse(\"Sheet1\")\n",
    "    \n",
    "    if catch_err_before_begin() == True:\n",
    "        #name is not working\n",
    "        if return_name() in df.values:\n",
    "            print(name,\"exists in Dataframe\")\n",
    "            \n",
    "            row = df.loc[df['name'] == name]\n",
    "            get_type_trainval = row.loc[:,\"type\"] == \"trainval\"\n",
    "\n",
    "            #if video belongs to trainval\n",
    "            if get_type_trainval.bool() == True: \n",
    "                %cd -q  ./datasets/ava/videos/trainval\n",
    "                upload_to_path(\"./STEP/dataset/ava/videos/trainval\")\n",
    "            else:\n",
    "                %cd -q  ./datasets/ava/videos/test\n",
    "                upload_to_path(\"./STEP/dataset/ava/videos/test\")\n",
    "            %cd -q ../\n",
    "            %cd -q ../\n",
    "            %cd -q ../\n",
    "            %cd -q ../\n",
    "        else:\n",
    "            print(\" video file does not belong to STEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dgd963k-3DR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_list = []\n",
    "if current_pipeline == \"TSU\":\n",
    "    FOLDER_PATH = 'input_files'\n",
    "    ROOT_PATH = './data'\n",
    "\n",
    "    file_list = os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH))\n",
    "elif current_pipeline == \"STEP\":\n",
    "    for root, dirs, files in os.walk(\"./datasets/ava/videos\"):\n",
    "        for file in files:\n",
    "            #append the file name to the list\n",
    "            file_list.append(file)\n",
    "            \n",
    "elif current_pipeline == \"MSTCT\":\n",
    "    #change to whatever is needed\n",
    "    ROOT_PATH = './dataset'\n",
    "    file_list = os.listdir(os.path.join(ROOT_PATH))\n",
    "\n",
    "w = widgets.Dropdown(\n",
    "    options= file_list,\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        change['new']\n",
    "\n",
    "w.observe(on_change)\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB0moBiBCpVL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if current_pipeline == \"TSU\":\n",
    "    video_path = './data/input_files/%' + w.value\n",
    "elif current_pipeline == \"STEP\":\n",
    "    if os.path.isfile('./datasets/ava/videos/train_vid/' + w.value):\n",
    "        video_path = './datasets/ava/videos/train_vid/' + w.value\n",
    "    elif os.path.isfile('./datasets/ava/videos/val_vid/' + w.value):\n",
    "        video_path = './datasets/ava/videos/val_vid/' + w.value\n",
    "\n",
    "locate= \"\"\n",
    "\n",
    "for i in video_path: \n",
    "    if i=='%':\n",
    "        pass\n",
    "    else:\n",
    "        locate+=i \n",
    "\n",
    "def show_video(locate, video_width = 500):\n",
    "    video_file = open(locate, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
    "  \n",
    "#Video(locate, embed=True, width=320, height=320)\n",
    "show_video(locate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction (TSU)\n",
    "\n",
    "This section allows extracting of features using TSU pipeline by selecting the video from the list loaded in the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction prequisite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate training and testing lists base on cs json file\n",
    "def unextracted_list():\n",
    "    full_vid_list = \"./Toyota_Smarthome/pipline/data/smarthome_CS_51.json\"\n",
    "    full_vid_file = open(full_vid_list)\n",
    "    full_vid_json = json.load(full_vid_file)\n",
    "    \n",
    "    unextracted_vid_list = []\n",
    "    for vid_key in full_vid_json:\n",
    "        unextracted_vid_list.append(vid_key)\n",
    "        \n",
    "    extracted_vid_list = \"./Toyota_Smarthome/pipline/data/i3d_CS.json\"\n",
    "    extracted_vid_file = open(extracted_vid_list)\n",
    "    extracted_vid_json = json.load(extracted_vid_file)\n",
    "    \n",
    "    for extracted_vid_key in extracted_vid_json:\n",
    "        if extracted_vid_key[:-4] in unextracted_vid_list:\n",
    "            unextracted_vid_list.remove(extracted_vid_key[:-4])\n",
    "    unextracted_vid_list.sort()\n",
    "    return unextracted_vid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select video to extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63967bf5e1b497699df4ce39ed75514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Videos:', options=('P02T02C03', 'P02T02C06', 'P02T02C07', 'P02T03C07', 'P02T04C04'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb559f0baf84f688deb522159522587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extract', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking for selected video's npy\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\GitHub\\ict3104-p2-team08\\video_features\\test.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mextractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExtractI3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Extract features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\ict3104-p2-team08\\video_features\\models2\\i3d\\extract_i3d.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_feat_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'fps'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'timestamps_ms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname2module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\ict3104-p2-team08\\video_features\\models2\\i3d\\extract_i3d.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mi3d_stream_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mI3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi3d_classes_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mi3d_stream_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi3d_weights_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mi3d_stream_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi3d_stream_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[0mi3d_stream_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mi3d_stream_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi3d_stream_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    923\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    924\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 925\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "unextracted_vid_sel = widgets.SelectMultiple(\n",
    "options=unextracted_list(),\n",
    "rows=10,\n",
    "description='Videos:',\n",
    "disabled=False\n",
    ")\n",
    "\n",
    "display(unextracted_vid_sel)\n",
    "\n",
    "selected_videos = []\n",
    "\n",
    "extract_but = widgets.Button(description = 'Extract')\n",
    "def on_ext_button_clicked(b):\n",
    "    combine_dataset = unextracted_vid_sel.value\n",
    "    selected_videos = list(combine_dataset)\n",
    "    selected_videos_string=(','.join(selected_videos))\n",
    "    #extract features as npy\n",
    "    %cd -q video_features\n",
    "    %pwd\n",
    "    %run -i ./test.py -videosToExtract={selected_videos_string}\n",
    "    %cd -q ..\n",
    "\n",
    "\n",
    "extract_but.on_click(on_ext_button_clicked)\n",
    "\n",
    "display(extract_but)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pbFLqFs08xy"
   },
   "source": [
    "## Inference with pretrained model\n",
    "\n",
    "This section allows generation of caption videos which can also be done on models you have trained which would automatically appear in the dropdown input below. To generate the videos, follow these steps: \n",
    "* Select a pre-trained model \n",
    "* Choose a video input\n",
    "* generate video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaf4f2caa8a4707a169769c43060aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('PDAN',), value='PDAN')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if current_pipeline == \"TSU\":\n",
    "    MODEL_PATH = 'models'\n",
    "    MODEL_ROOT_PATH = './Toyota_Smarthome/pipline/'\n",
    "\n",
    "    file_list = os.listdir(os.path.join(MODEL_ROOT_PATH, MODEL_PATH))\n",
    "elif current_pipeline == \"STEP\":\n",
    "    MODEL_PATH = ''\n",
    "    MODEL_ROOT_PATH = './pretrained'\n",
    "\n",
    "    file_list = os.listdir(os.path.join(MODEL_ROOT_PATH, MODEL_PATH))\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=file_list,\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        change['new']\n",
    "\n",
    "model_dropdown.observe(on_change)\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d6bc12c7024971bc41d92ec5d28c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Video:', options=('P02T01C06.mp4', 'P02T01C07.mp4', 'P02T02C03.mp4', 'P02T02C06.mp4', 'P…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VIDEO_PATH = 'input_files'\n",
    "\n",
    "model_name = model_dropdown.value\n",
    "    \n",
    "#JSON_FILEPATH = \"./Toyota_Smarthome/pipline/model_videos.json\"\n",
    "#f = open(JSON_FILEPATH)\n",
    "#data = json.load(f)\n",
    "#testable_video = data[model_name]\n",
    "#testable_video = [video + \".mp4\" for video in testable_video]\n",
    "#print(testable_video)\n",
    "\n",
    "if current_pipeline == \"TSU\":\n",
    "    VIDEO_ROOT_PATH = './data/'\n",
    "    full_testable_video = os.listdir(os.path.join(VIDEO_ROOT_PATH, VIDEO_PATH))\n",
    "    \n",
    "    if model_name == \"PDAN\":\n",
    "        file_list = full_testable_video\n",
    "    else:\n",
    "        JSON_FILEPATH = \"./Toyota_Smarthome/pipline/data/\" + model_name + \"_CS.json\"\n",
    "        f = open(JSON_FILEPATH)\n",
    "        data = json.load(f)\n",
    "        testable_video = []\n",
    "        for video_key, video_values in data.items():\n",
    "            if video_values[\"subset\"] == \"testing\":\n",
    "                testable_video.append(video_key)\n",
    "        file_list = testable_video\n",
    "elif current_pipeline == \"STEP\":\n",
    "    full_testable_video = os.listdir(os.path.join('../data/', VIDEO_PATH))\n",
    "    file_list = full_testable_video\n",
    "\n",
    "video_dropdown = widgets.Dropdown(\n",
    "    options=file_list,\n",
    "    description='Video:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        change['new']\n",
    "\n",
    "video_dropdown.observe(on_change)\n",
    "display(video_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4D84kQqh0-1_",
    "outputId": "a17fda50-ff6b-4f13-c13f-ad5dcb9a5118",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536/536 [00:11<00:00, 45.63it/s]\n",
      "100%|██████████| 536/536 [00:04<00:00, 111.70it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\GitHub\\ict3104-p2-team08\\Toyota_Smarthome\\pipline\\test2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"False\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;31m# entire model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;31m# weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[1;31m# model.load_state_dict(torch.load(str(args.load_model)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    999\u001b[0m         \u001b[1;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[1;32m-> 1001\u001b[1;33m             \u001b[0mwrap_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1002\u001b[0m             dtype=dtype)\n\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[0;32m    137\u001b[0m                            \u001b[1;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                            \u001b[1;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,./Toyota_Smarthome/pipline/video_output/P02T01C06_caption.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if current_pipeline == \"TSU\":\n",
    "    video_selected = video_dropdown.value.replace('.mp4','')\n",
    "    model_selected = model_dropdown.value\n",
    "    model_path = \"./Toyota_Smarthome/pipline/models/\" + model_selected\n",
    "    %run -i ./Toyota_Smarthome/pipline/test2.py -videofile={video_selected} -load_model={model_path} -name={model_selected}\n",
    "    video_path = \"./Toyota_Smarthome/pipline/video_output/\" + video_selected + \"_caption.mp4\"\n",
    "elif current_pipeline == \"STEP\":\n",
    "    %run -i ./extract_frames.py --video_name={video_dropdown.value}\n",
    "    %run -i ./demo.py --model_name={model_dropdown.value} --video_name={video_dropdown.value}\n",
    "    video_path = \"./video_output/\" + video_dropdown.value\n",
    "Video(video_path, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyUzIqvd3rue"
   },
   "source": [
    "## Training of new model\n",
    "\n",
    "This section allows the training/testing of models of your own creation. Follow the steps below to train your own model starting the training prerequisite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to generate training and testing lists base on cs json file\n",
    "def training_testing_list():\n",
    "    split_setting = \"CS\"\n",
    "    cs_filepath = \"./Toyota_Smarthome/pipline/data/i3d_CS.json\"\n",
    "    if split_setting == \"CS\":\n",
    "        split_setting_file = open(cs_filepath)\n",
    "        split_setting_json = json.load(split_setting_file)\n",
    "    training_list = []\n",
    "    testing_list = []\n",
    "    for video_key, video_data in split_setting_json.items():\n",
    "        if video_data[\"subset\"] == \"training\":\n",
    "            training_list.append(video_key[:-4] + \".mp4\")\n",
    "        elif video_data[\"subset\"] == \"testing\":\n",
    "            testing_list.append(video_key[:-4] + \".mp4\")\n",
    "    #sort lists according to naming\n",
    "    training_list.sort()\n",
    "    testing_list.sort()\n",
    "    return training_list, testing_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose datasets for training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yZ3NT9Et3wnT",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfaee45bc7540afa22e245cab45dd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='% of Training video (Total: 1)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5152d094c6ab4ccc93b7c7fb49a0390e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=1, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226f000215f04a828acb1b5cdc7c5a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='% of Training video (Total: 3)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c710ea1aecf441b926c435b8b5d8d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=1, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d4db0812644d5b8d9aa02e12484304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Enter', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if current_pipeline == \"TSU\":\n",
    "    training_list, testing_list = training_testing_list()\n",
    "elif current_pipeline == \"STEP\":\n",
    "    training_list = os.listdir(os.path.join(\"./datasets/ava/videos/\", \"train_vid\"))\n",
    "    testing_list = os.listdir(os.path.join(\"./datasets/ava/videos/\", \"val_vid\"))\n",
    "\n",
    "\n",
    "training_box = widgets.BoundedIntText(value=1, min=1, max=100)\n",
    "testing_box = widgets.BoundedIntText(value=1, min=1, max=100)\n",
    "\n",
    "tot_train_vid = len(training_list)\n",
    "tot_test_vid = len(testing_list)\n",
    "\n",
    "training_label = widgets.Label(value=\"% of Training video (Total: \" + str(tot_train_vid) + \")\")\n",
    "testing_label = widgets.Label(value=\"% of Testing video (Total: \" + str(tot_test_vid) + \")\")\n",
    "\n",
    "display(training_label)\n",
    "display(training_box)\n",
    "display(testing_label)\n",
    "display(testing_box)\n",
    "\n",
    "###new\n",
    "\n",
    "selected_videos = []\n",
    "\n",
    "but = widgets.Button(description = 'Enter')\n",
    "if current_pipeline == \"TSU\":\n",
    "    def on_button_clicked(b):\n",
    "        global selected_videos_string\n",
    "        sel_num_train = int((training_box.value/100) * tot_train_vid)\n",
    "        if sel_num_train < 1:\n",
    "            sel_num_train = 1\n",
    "        sel_num_test = int((testing_box.value/100) * tot_test_vid)\n",
    "        if sel_num_test < 1:\n",
    "            sel_num_test = 1\n",
    "        train_list = training_list[0:sel_num_train]\n",
    "        test_list = testing_list[0:sel_num_test]\n",
    "        combine_dataset = train_list + test_list\n",
    "        selected_videos = list(combine_dataset)\n",
    "        for i in range(len(selected_videos)):\n",
    "            selected_videos[i] = selected_videos[i].replace('.mp4','')\n",
    "        selected_videos_string=(','.join(selected_videos))\n",
    "elif current_pipeline == \"STEP\":\n",
    "    def on_button_clicked(b):\n",
    "        global selected_train_videos\n",
    "        global selected_test_videos\n",
    "        sel_num_train = int((training_box.value/100) * tot_train_vid)\n",
    "        if sel_num_train < 1:\n",
    "            sel_num_train = 1\n",
    "        sel_num_test = int((testing_box.value/100) * tot_test_vid)\n",
    "        if sel_num_test < 1:\n",
    "            sel_num_test = 1\n",
    "        train_list = training_list[0:sel_num_train]\n",
    "        test_list = testing_list[0:sel_num_test]\n",
    "        for i in range(len(train_list)):\n",
    "            train_list[i] = train_list[i].replace('.mp4','')\n",
    "            train_list[i] = train_list[i].replace('.mkv','')\n",
    "            train_list[i] = train_list[i].replace('.webm','')\n",
    "        for i in range(len(test_list)):\n",
    "            test_list[i] = test_list[i].replace('.mp4','')\n",
    "            test_list[i] = test_list[i].replace('.mkv','')\n",
    "            test_list[i] = test_list[i].replace('.webm','')\n",
    "        selected_train_videos=(','.join(train_list))\n",
    "        selected_test_videos=(','.join(test_list))\n",
    "\n",
    "but.on_click(on_button_clicked)\n",
    "\n",
    "display(but)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model name, Batch size, Epochs\n",
    "After training your model, you will be able to use it in the generation of features on a selected video of your choice. Thus, naming your model is crucial. \n",
    "\n",
    "**Do note that the maximum batch size is 4 and epochs is 1000.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546a384494104d049dbe1c3f3ea8da8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model name:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf61a0ff774453f8a8b1602928084f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=2, description='Batch size:', max=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cbbcf1be9b4a47a8c5db181aa8fd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=1000, description='Epochs:', max=1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29287fdfb234f71b678e127f0cdf97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Learning Rate:'), BoundedIntText(value=1)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbafa90d263343a69e60954dc9fa9d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Enter', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp = widgets.Text(description='Model name:')\n",
    "batch_size = widgets.BoundedIntText(description='Batch size:',\n",
    "                                   value=2,\n",
    "                                   max=4)\n",
    "epochs = widgets.BoundedIntText(description='Epochs:',\n",
    "                               value=1000,\n",
    "                               max=1000)\n",
    "learning_rate = widgets.BoundedIntText(value=1)\n",
    "button = widgets.Button(description=\"Enter\")\n",
    "\n",
    "button = widgets.Button(description=\"Enter\")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    inp.value\n",
    "    batch_size.value\n",
    "    epochs.value\n",
    "    learning_rate.value\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(inp)\n",
    "display(batch_size)\n",
    "display(epochs)\n",
    "display(widgets.HBox([widgets.Label(value=\"Learning Rate:\"), learning_rate]))\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global current_pipeline\n",
    "\n",
    "model_name = inp.value\n",
    "epochValue = (epochs.value)\n",
    "batchSize = str(batch_size.value)\n",
    "lrValue = learning_rate.value\n",
    "\n",
    "if current_pipeline == \"TSU\":\n",
    "    %run -i ./Toyota_Smarthome/pipline/train.py -video_train_test={selected_videos_string} -model=PDAN -name={model_name} -batch_size={batchSize} -epoch={epochValue} -lr={lrValue}\n",
    "    %wandb {wandb_username.value}/ICT3104_project\n",
    "elif current_pipeline == \"STEP\":\n",
    "    %run -i ./overwrite_csv_trainval.py --video_train={selected_train_videos} --video_test={selected_test_videos}\n",
    "    %run -i ./scripts/generate_label.py ./datasets/ava/label/new_train.csv\n",
    "    %run -i ./scripts/generate_label.py ./datasets/ava/label/new_val.csv\n",
    "    %run -i ./train.py --model_name={model_name} --batch_size={batchSize} --max_epochs={epochValue} --base_lr={lrValue}\n",
    "    %wandb {wandb_username.value}/ICT3104_project_STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4pbFLqFs08xy"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
