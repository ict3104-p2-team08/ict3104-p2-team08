{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ict3104-p2-team08/ict3104-p2-team08/blob/inference_feat/3104_T08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yJLCy-Wv_Rd"
   },
   "source": [
    "# Activity Detection Notebook\n",
    "Welcome to the ICT3104 Team08 activity Detection notebook! You'll be taken through the steps of running a pretrained activity detection model on videos and use your own trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyKsO3crw1K5"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "This section provides the setting up of the environment and dependencies required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heoGQJ7NyBiM",
    "outputId": "a2fa6fd7-2874-4e82-9914-1d335a7735cd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch -f https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install torchaudio -f https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install torchsummary==1.5.1 torchtext==0.13.1 torchvision -f https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FQXemWrcw4pE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown, widgets\n",
    "import os\n",
    "from IPython.display import HTML, Video, clear_output\n",
    "from base64 import b64encode\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Wandb\n",
    "Create an account and Login with Wandb to view analytical charts of the activity detection using the following link: https://wandb.ai/site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_username = widgets.Text(description='Username:')\n",
    "display(wandb_username)\n",
    "\n",
    "wandb_key = widgets.Password(\n",
    "    description='Api key:',\n",
    "    disabled=False)\n",
    "display(wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Login\")\n",
    "\n",
    "display(button)\n",
    "\n",
    "def on_button_click(b):\n",
    "    !wandb login {wandb_key.value}\n",
    "\n",
    "button.on_click(on_button_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcEvKDf2wVL2"
   },
   "source": [
    "## Toggling of pipelines\n",
    "\n",
    "Here we will choose which feature extraction pipeline dependencies we will use between TSU, STEP and MCTCT.\n",
    "The selected pipeline will be loaded authomatically whereby changes can be made at any moment in the next cell and normal execution of the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lb4Grr9wuhWu",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba57b17a3524202802c7020022ec172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Pipelines:', options=('TSU', 'STEP', 'MSTCT'), value='TSU')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current selected pipeline: MSTCT \r"
     ]
    }
   ],
   "source": [
    "current_pipeline = \"TSU\"\n",
    "rb = widgets.RadioButtons(\n",
    "    options=['TSU', 'STEP', 'MSTCT'],\n",
    "    description='Pipelines:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    global current_pipeline\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        if current_pipeline != change['new']:\n",
    "            if change['new'] == \"TSU\":\n",
    "                %cd -q ../\n",
    "            elif change['new'] == \"STEP\":\n",
    "                if os.path.basename(os.path.normpath(os.getcwd())) ==\"MSTCT\":\n",
    "                    %cd -q ..\n",
    "                    %cd -q  ./STEP\n",
    "                else:\n",
    "                    %cd -q  ./STEP\n",
    "            elif change['new'] == \"MSTCT\":\n",
    "                if os.path.basename(os.path.normpath(os.getcwd())) ==\"STEP\":\n",
    "                    %cd -q ../\n",
    "                    %cd -q ./MSTCT\n",
    "                else:\n",
    "                    %cd -q ./MSTCT\n",
    "        current_pipeline = change['new']\n",
    "        print(\"Current selected pipeline: {:<6}\".format(current_pipeline), end='\\r')\n",
    "\n",
    "rb.observe(on_change)\n",
    "\n",
    "display(rb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importing Datasets\n",
    "More videos from the followiing link can be added using the steps below in the respective folders: https://github.com/cvdfoundation/ava-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TSU (RGB variant only)\n",
    "\n",
    "#### Importing TSU provided Video/CSV files.\n",
    "\n",
    "##### For video files\n",
    "Step 1. Go to /ict3104-p2-team08/data/input_files/ directory\n",
    "<br>\n",
    "Step 2. Add in video files provided by TSU (Feature extraction/inference needed)\n",
    "##### For CSV files\n",
    "Step 1. Go to /ict3104-p2-team08/data/input_csv/ directory\n",
    "<br>\n",
    "Step 2. Add in CSV files provided by TSU (Inference needed to show actual annotated result)\n",
    "\n",
    "#### Importing TSU provided NPY files.\n",
    "Step 1. Go to /Toyota_Smarthome/pipline/data/RGB_i3d_16frames_64000_SSD/ directory\n",
    "<br>\n",
    "Step 2. Add in all npy files provided by TSU\n",
    "\n",
    "#### Importing V-iashin provided RGB NPY files.\n",
    "If you have already extracted the RGB NPY files do the following:\n",
    "<br>\n",
    "Step 1. Go to /ict3104-p2-team08/Toyota_Smarthome/pipline/data/RGB_v_iashin/ directory\n",
    "<br>\n",
    "Step 2. Add in the v-iashin extracted rgb npy files\n",
    "<br>\n",
    "If not extracted can make use of feature extraction section below (Note: feature extraction is only possible for TSU videos that can be found in <br> /ict3104-p2-team08/data/input_files/ directory)\n",
    "\n",
    "#### Importing TSU provided Pretrained model (PDAN).\n",
    "Step 1. Go to /ict3104-p2-team08/Toyota_Smarthome/pipline/models/ directory\n",
    "<br>\n",
    "Step 2. Add in PDAN provided by TSU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### STEP\n",
    "\n",
    "#### Importing STEP provided Video files.\n",
    "\n",
    "##### For video files\n",
    "##### Training video files\n",
    "Step 1. Go to /ict3104-p2-team08/STEP/datasets/ava/videos/train_vid/ directory\n",
    "<br>\n",
    "Step 2. Add in training video files provided by AVA\n",
    "##### Testing video files\n",
    "Step 1. Go to /ict3104-p2-team08/STEP/datasets/ava/videos/val_vid/ directory\n",
    "<br>\n",
    "Step 2. Add in validation video files provided by AVA\n",
    "\n",
    "#### Importing STEP provided Pretrained model (ava_step.pth).\n",
    "Step 1. Go to /ict3104-p2-team08/STEP/pretrained/ directory\n",
    "<br>\n",
    "Step 2. Add in ava_step.pth provided by AVA\n",
    "\n",
    "#### To extract video frames for newly added videos (for training/testing) in /ict3104-p2-team08/STEP/datasets/ava/videos/ directory, run the below cell (Note: Run section 2 Toggling of pipelines and check STEP radio button first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%run -i ./scripts/extract_clips.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voFOEyiP0WSt"
   },
   "source": [
    "## Data exploration\n",
    "\n",
    "This section load and display video data from the Toyota Smarthome (TSU) project\n",
    "\n",
    "### Jupyter -- Upload any files/datasets into the main folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploader = widgets.FileUpload(accept='.mp4',  multiple=True)\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd Documents/GitHub/ict3104-p2-team08\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = uploader.value\n",
    "import pandas as pd\n",
    "\n",
    "#depends on pipeline\n",
    "\n",
    "def return_name():\n",
    "    for name, file_info in uploader.value.items():\n",
    "        name_file = name\n",
    "    return name_file\n",
    "    \n",
    "    \n",
    "def catch_err_before_begin():\n",
    "    uploader.value\n",
    "    if len(uploader.value) == 0:\n",
    "        print(\"An exception occurred\")\n",
    "        print(\"possible reason - file too large to upload. consider manual uploading of dataset in jupyter notebook\")\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "    \n",
    "def upload_to_path(dir):\n",
    "    try:\n",
    "        for name, file_info in uploader.value.items():\n",
    "            with open (name, 'wb') as file:\n",
    "                file.write(file_info['content'])\n",
    "        print(\"successful upload of\", name, \"to\", dir)\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "        \n",
    "        \n",
    "        \n",
    "# uplaod videos to data/input_files for tsu.\n",
    "# return back to root dir\n",
    "if current_pipeline == \"TSU\":\n",
    "    if catch_err_before_begin() == True:\n",
    "        %cd -q  ./data/input_files\n",
    "        upload_to_path(\"./data/input_files\")\n",
    "        %cd -q ../\n",
    "        %cd -q ../\n",
    "    \n",
    "\n",
    "# uplaod videos to data/input_files for mstct.\n",
    "# return back to mstct dir\n",
    "if current_pipeline == \"MSTCT\":\n",
    "    if catch_err_before_begin() == True:\n",
    "        %cd -q ../\n",
    "        %cd -q  ./data/input_files\n",
    "        upload_to_path(\"./data/input_files\")\n",
    "        %cd -q ../\n",
    "        %cd -q ../\n",
    "        %cd -q ./MSTCT\n",
    "    \n",
    "            \n",
    "if current_pipeline == \"STEP\":\n",
    "    #check if trainval or test\n",
    "    xl = pd.ExcelFile(\"./datasets/ava/check.xlsx\")\n",
    "    df = xl.parse(\"Sheet1\")\n",
    "    \n",
    "    if catch_err_before_begin() == True:\n",
    "        #name is not working\n",
    "        if return_name() in df.values:\n",
    "            print(name,\"exists in Dataframe\")\n",
    "            \n",
    "            row = df.loc[df['name'] == name]\n",
    "            get_type_trainval = row.loc[:,\"type\"] == \"trainval\"\n",
    "\n",
    "            #if video belongs to trainval\n",
    "            if get_type_trainval.bool() == True: \n",
    "                %cd -q  ./datasets/ava/videos/trainval\n",
    "                upload_to_path(\"./STEP/dataset/ava/videos/trainval\")\n",
    "            else:\n",
    "                %cd -q  ./datasets/ava/videos/test\n",
    "                upload_to_path(\"./STEP/dataset/ava/videos/test\")\n",
    "            %cd -q ../\n",
    "            %cd -q ../\n",
    "            %cd -q ../\n",
    "            %cd -q ../\n",
    "        else:\n",
    "            print(\" video file does not belong to STEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dgd963k-3DR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_list = []\n",
    "if current_pipeline == \"TSU\":\n",
    "    FOLDER_PATH = 'input_files'\n",
    "    ROOT_PATH = './data'\n",
    "\n",
    "    file_list = os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH))\n",
    "elif current_pipeline == \"STEP\":\n",
    "    for root, dirs, files in os.walk(\"./datasets/ava/videos\"):\n",
    "        for file in files:\n",
    "            #append the file name to the list\n",
    "            file_list.append(file)\n",
    "            \n",
    "elif current_pipeline == \"MSTCT\":\n",
    "    #change to whatever is needed\n",
    "    FOLDER_PATH = 'input_files'\n",
    "    ROOT_PATH = '.././data/'#get the list of files\n",
    "\n",
    "    file_list = os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH))\n",
    "\n",
    "w = widgets.Dropdown(\n",
    "    options= file_list,\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        change['new']\n",
    "\n",
    "w.observe(on_change)\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB0moBiBCpVL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if current_pipeline == \"TSU\":\n",
    "    video_path = './data/input_files/%' + w.value\n",
    "elif current_pipeline == \"STEP\":\n",
    "    if os.path.isfile('./datasets/ava/videos/train_vid/' + w.value):\n",
    "        video_path = './datasets/ava/videos/train_vid/' + w.value\n",
    "    elif os.path.isfile('./datasets/ava/videos/val_vid/' + w.value):\n",
    "        video_path = './datasets/ava/videos/val_vid/' + w.value\n",
    "if current_pipeline == \"MSTCT\":\n",
    "    video_path = '../data/input_files/%' + w.value\n",
    "\n",
    "locate= \"\"\n",
    "\n",
    "for i in video_path: \n",
    "    if i=='%':\n",
    "        pass\n",
    "    else:\n",
    "        locate+=i \n",
    "\n",
    "def show_video(locate, video_width = 500):\n",
    "    video_file = open(locate, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
    "  \n",
    "#Video(locate, embed=True, width=320, height=320)\n",
    "show_video(locate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction (TSU)\n",
    "\n",
    "This section allows extracting of features using TSU pipeline by selecting the video from the list loaded in the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction prequisite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate training and testing lists base on cs json file\n",
    "def unextracted_list():\n",
    "    full_vid_list = \"./Toyota_Smarthome/pipline/data/smarthome_CS_51.json\"\n",
    "    full_vid_file = open(full_vid_list)\n",
    "    full_vid_json = json.load(full_vid_file)\n",
    "    \n",
    "    unextracted_vid_list = []\n",
    "    for vid_key in full_vid_json:\n",
    "        unextracted_vid_list.append(vid_key)\n",
    "        \n",
    "    extracted_vid_list = \"./Toyota_Smarthome/pipline/data/i3d_CS.json\"\n",
    "    extracted_vid_file = open(extracted_vid_list)\n",
    "    extracted_vid_json = json.load(extracted_vid_file)\n",
    "    \n",
    "    for extracted_vid_key in extracted_vid_json:\n",
    "        if extracted_vid_key[:-4] in unextracted_vid_list:\n",
    "            unextracted_vid_list.remove(extracted_vid_key[:-4])\n",
    "    unextracted_vid_list.sort()\n",
    "    return unextracted_vid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select video to extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unextracted_vid_sel = widgets.SelectMultiple(\n",
    "options=unextracted_list(),\n",
    "rows=10,\n",
    "description='Videos:',\n",
    "disabled=False.sh\n",
    ")\n",
    "\n",
    "display(unextracted_vid_sel)\n",
    "\n",
    "selected_videos = []\n",
    "\n",
    "extract_but = widgets.Button(description = 'Extract')\n",
    "def on_ext_button_clicked(b):\n",
    "    combine_dataset = unextracted_vid_sel.value\n",
    "    selected_videos = list(combine_dataset)\n",
    "    selected_videos_string=(','.join(selected_videos))\n",
    "    #extract features as npy\n",
    "    %cd -q video_features\n",
    "    %pwd\n",
    "    %run -i ./test.py -videosToExtract={selected_videos_string}\n",
    "    %cd -q ..\n",
    "\n",
    "\n",
    "extract_but.on_click(on_ext_button_clicked)\n",
    "\n",
    "display(extract_but)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for MSTCT\n",
    "This section allows the extraction for MSTCT pipeline using charades dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #function to generate training and testing lists base on cs json file\n",
    "# def unextracted_list_MSTCT():\n",
    "#     full_vid_list = \"./data/charades.json\"\n",
    "#     full_vid_file = open(full_vid_list)\n",
    "#     full_vid_json = json.load(full_vid_file)\n",
    "    \n",
    "#     unextracted_vid_list = []\n",
    "#     for vid_key in full_vid_json:\n",
    "#         unextracted_vid_list.append(vid_key)\n",
    "        \n",
    "#     extracted_vid_list = \"./data/i3d_extracted.json\"\n",
    "#     extracted_vid_file = open(extracted_vid_list)\n",
    "#     extracted_vid_json = json.load(extracted_vid_file)\n",
    "    \n",
    "#     for extracted_vid_key in extracted_vid_json:\n",
    "#         print(extracted_vid_key)\n",
    "#         if extracted_vid_key in unextracted_vid_list:\n",
    "#             unextracted_vi_list.remove(extracted_vid_key)\n",
    "#     unextracted_vid_list.sort()\n",
    "#     return unextracted_vid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unextracted_vid_sel = widgets.SelectMultiple(\n",
    "# options=unextracted_list_MSTCT(),\n",
    "# rows=10,\n",
    "# description='Videos:',\n",
    "# disabled=False\n",
    "# )\n",
    "\n",
    "# display(unextracted_vid_sel)\n",
    "\n",
    "# selected_videos = []\n",
    "\n",
    "# extract_but = widgets.Button(description = 'Extract')\n",
    "# def on_ext_button_clicked(b):\n",
    "#     combine_dataset = unextracted_vid_sel.value\n",
    "#     selected_videos = list(combine_dataset)\n",
    "#     selected_videos_string=(','.join(selected_videos))\n",
    "#     #extract features as npy\n",
    "#     mode = \"rgb\"\n",
    "#     root = (\"./dataset/Test_Charades/\" + selected_videos_string)\n",
    "#     print(root)\n",
    "#     split = \"./data/charades.json\"\n",
    "#     load_model = \"../MS_video_features/pytorch-i3d/models/rgb_charades.pt\"\n",
    "#     save_directory = \"./dataset/\"\n",
    "#     %run -i ../MS_video_features/pytorch-i3d/extract_features.py -mode={mode} -root={root} -load_model={load_model} -save_dir=\n",
    "\n",
    "\n",
    "# extract_but.on_click(on_ext_button_clicked)\n",
    "\n",
    "# display(extract_but)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "mode = \"rgb\"\n",
    "root = \"./dataset/Demo_Charades\"\n",
    "split = \"./data/charades.json\"\n",
    "load_model = \"../MS_video_features/pytorch-i3d/models/rgb_charades.pt\"\n",
    "save_directory = \"./dataset/\"\n",
    "%run -i ../MS_video_features/pytorch-i3d/extract_features.py -mode={mode} -root={root} -load_model={load_model} -save_dir={save_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pbFLqFs08xy"
   },
   "source": [
    "## Inference with pretrained model\n",
    "\n",
    "This section allows generation of caption videos which can also be done on models you have trained which would automatically appear in the dropdown input below. To generate the videos, follow these steps: \n",
    "* Select a pre-trained model \n",
    "* Choose a video input\n",
    "* generate video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if current_pipeline == \"TSU\":\n",
    "    MODEL_PATH = 'models'\n",
    "    MODEL_ROOT_PATH = './Toyota_Smarthome/pipline/'\n",
    "\n",
    "    file_list = os.listdir(os.path.join(MODEL_ROOT_PATH, MODEL_PATH))\n",
    "elif current_pipeline == \"STEP\":\n",
    "    MODEL_PATH = ''\n",
    "    MODEL_ROOT_PATH = './pretrained'\n",
    "\n",
    "    file_list = os.listdir(os.path.join(MODEL_ROOT_PATH, MODEL_PATH))\n",
    "elif current_pipeline == \"MSTCT\":\n",
    "    MODEL_PATH = ''\n",
    "    MODEL_ROOT_PATH = './MSTCT'\n",
    "\n",
    "    file_list = os.listdir(os.path.join(MODEL_ROOT_PATH, MODEL_PATH))\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=file_list,\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        change['new']\n",
    "\n",
    "model_dropdown.observe(on_change)\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VIDEO_PATH = 'input_files'\n",
    "\n",
    "model_name = model_dropdown.value\n",
    "    \n",
    "#JSON_FILEPATH = \"./Toyota_Smarthome/pipline/model_videos.json\"\n",
    "#f = open(JSON_FILEPATH)\n",
    "#data = json.load(f)\n",
    "#testable_video = data[model_name]\n",
    "#testable_video = [video + \".mp4\" for video in testable_video]\n",
    "#print(testable_video)\n",
    "\n",
    "if current_pipeline == \"TSU\":\n",
    "    VIDEO_ROOT_PATH = './data/'\n",
    "    full_testable_video = os.listdir(os.path.join(VIDEO_ROOT_PATH, VIDEO_PATH))\n",
    "    \n",
    "    if model_name == \"PDAN\":\n",
    "        file_list = full_testable_video\n",
    "    else:\n",
    "        JSON_FILEPATH = \"./Toyota_Smarthome/pipline/data/\" + model_name + \"_CS.json\"\n",
    "        f = open(JSON_FILEPATH)\n",
    "        data = json.load(f)\n",
    "        testable_video = []\n",
    "        for video_key, video_values in data.items():\n",
    "            if video_values[\"subset\"] == \"testing\":\n",
    "                testable_video.append(video_key)\n",
    "        file_list = testable_video\n",
    "elif current_pipeline == \"STEP\":\n",
    "    full_testable_video = os.listdir(os.path.join('../data/', VIDEO_PATH))\n",
    "    file_list = full_testable_video\n",
    "elif current_pipeline == \"MSTCT\":\n",
    "    VIDEO_ROOT_PATH = '.././data/'\n",
    "    full_testable_video = os.listdir(os.path.join(VIDEO_ROOT_PATH, VIDEO_PATH))\n",
    "    file_list = full_testable_video\n",
    "    \n",
    "video_dropdown = widgets.Dropdown(\n",
    "    options=file_list,\n",
    "    description='Video:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        change['new']\n",
    "\n",
    "video_dropdown.observe(on_change)\n",
    "display(video_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4D84kQqh0-1_",
    "outputId": "a17fda50-ff6b-4f13-c13f-ad5dcb9a5118",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if current_pipeline == \"TSU\":\n",
    "    video_selected = video_dropdown.value.replace('.mp4','')\n",
    "    model_selected = model_dropdown.value\n",
    "    model_path = \"./Toyota_Smarthome/pipline/models/\" + model_selected\n",
    "    %run -i ./Toyota_Smarthome/pipline/test2.py -videofile={video_selected} -load_model={model_path} -name={model_selected}\n",
    "    video_path = \"./Toyota_Smarthome/pipline/video_output/\" + video_selected + \"_caption.mp4\"\n",
    "elif current_pipeline == \"STEP\":\n",
    "    %run -i ./extract_frames.py --video_name={video_dropdown.value}\n",
    "    %run -i ./demo.py --model_name={model_dropdown.value} --video_name={video_dropdown.value}\n",
    "    video_path = \"./video_output/\" + video_dropdown.value\n",
    "elif current_pipeline == \"MSTCT\":\n",
    "    model_selected = model_dropdown.value\n",
    "    load_model = \"./MSTCT/\" + model_dropdown.value\n",
    "    %run -i ./train.py -load_model={load_model} \n",
    "    video_path = \"./video_output/\" + video_dropdown.value\n",
    "    \n",
    "Video(video_path, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyUzIqvd3rue"
   },
   "source": [
    "## Training of new model\n",
    "\n",
    "This section allows the training/testing of models of your own creation. Follow the steps below to train your own model starting the training prerequisite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training prerequisite -- TSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to generate training and testing lists base on cs json file\n",
    "def training_testing_list():\n",
    "    split_setting = \"CS\"\n",
    "    cs_filepath = \"./data/charades.json\"\n",
    "    if split_setting == \"CS\":\n",
    "        split_setting_file = open(cs_filepath)\n",
    "        split_setting_json = json.load(split_setting_file)\n",
    "    training_list = []\n",
    "    testing_list = []\n",
    "    for video_key, video_data in split_setting_json.items():\n",
    "        if video_data[\"subset\"] == \"training\":\n",
    "            training_list.append(video_key[:-4] + \".mp4\")\n",
    "        elif video_data[\"subset\"] == \"testing\":\n",
    "            testing_list.append(video_key[:-4] + \".mp4\")\n",
    "    #sort lists according to naming\n",
    "    training_list.sort()\n",
    "    testing_list.sort()\n",
    "    return training_list, testing_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training prerequisite -- MSTCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate training and testing lists base on cs json file\n",
    "def training_testing_list():\n",
    "    charade_path = \"./data/i3d_extracted.json\"\n",
    "    split_setting_file = open(charade_path)\n",
    "    split_setting_json = json.load(split_setting_file)\n",
    "    training_list = []\n",
    "    testing_list = []\n",
    "    for video_key, video_data in split_setting_json.items():\n",
    "        if video_data[\"subset\"] == \"training\":\n",
    "            training_list.append(video_key[:-4] + \".mp4\")\n",
    "        elif video_data[\"subset\"] == \"testing\":\n",
    "            testing_list.append(video_key[:-4] + \".mp4\")\n",
    "    #sort lists according to naming\n",
    "    training_list.sort()\n",
    "    testing_list.sort()\n",
    "    return training_list, testing_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose datasets for training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yZ3NT9Et3wnT",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a720d75f61415d89c3b0f254b3d91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='% of Training video (Total: 1)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5c78a0ad3c4575b362ace2cc1d41aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=1, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eaf5882e1874d82b617789003ec071b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='% of Testing video (Total: 1)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8655f05d6ab44069aee733c337813f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=1, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e030f7d3d0c841f88ddc5a2d3bd2aada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Enter', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if current_pipeline == \"TSU\":\n",
    "    training_list, testing_list = training_testing_list()\n",
    "elif current_pipeline == \"STEP\":\n",
    "    training_list = os.listdir(os.path.join(\"./datasets/ava/videos/\", \"train_vid\"))\n",
    "    testing_list = os.listdir(os.path.join(\"./datasets/ava/videos/\", \"val_vid\"))\n",
    "if current_pipeline == \"MSTCT\":\n",
    "    training_list, testing_list = training_testing_list()\n",
    "\n",
    "\n",
    "training_box = widgets.BoundedIntText(value=1, min=1, max=100)\n",
    "testing_box = widgets.BoundedIntText(value=1, min=1, max=100)\n",
    "\n",
    "tot_train_vid = len(training_list)\n",
    "tot_test_vid = len(testing_list)\n",
    "\n",
    "training_label = widgets.Label(value=\"% of Training video (Total: \" + str(tot_train_vid) + \")\")\n",
    "testing_label = widgets.Label(value=\"% of Testing video (Total: \" + str(tot_test_vid) + \")\")\n",
    "\n",
    "display(training_label)\n",
    "display(training_box)\n",
    "display(testing_label)\n",
    "display(testing_box)\n",
    "\n",
    "#new criteria\n",
    "\n",
    "selected_videos = []\n",
    "\n",
    "but = widgets.Button(description = 'Enter')\n",
    "if current_pipeline == \"TSU\":\n",
    "    def on_button_clicked(b):\n",
    "        global selected_videos_string\n",
    "        sel_num_train = int((training_box.value/100) * tot_train_vid)\n",
    "        if sel_num_train < 1:\n",
    "            sel_num_train = 1\n",
    "        sel_num_test = int((testing_box.value/100) * tot_test_vid)\n",
    "        if sel_num_test < 1:\n",
    "            sel_num_test = 1\n",
    "        train_list = training_list[0:sel_num_train]\n",
    "        test_list = testing_list[0:sel_num_test]\n",
    "        combine_dataset = train_list + test_list\n",
    "        selected_videos = list(combine_dataset)\n",
    "        for i in range(len(selected_videos)):\n",
    "            selected_videos[i] = selected_videos[i].replace('.mp4','')\n",
    "        selected_videos_string=(','.join(selected_videos))\n",
    "elif current_pipeline == \"STEP\":\n",
    "    def on_button_clicked(b):\n",
    "        global selected_train_videos\n",
    "        global selected_test_videos\n",
    "        sel_num_train = int((training_box.value/100) * tot_train_vid)\n",
    "        if sel_num_train < 1:\n",
    "            sel_num_train = 1\n",
    "        sel_num_test = int((testing_box.value/100) * tot_test_vid)\n",
    "        if sel_num_test < 1:\n",
    "            sel_num_test = 1\n",
    "        train_list = training_list[0:sel_num_train]\n",
    "        test_list = testing_list[0:sel_num_test]\n",
    "        for i in range(len(train_list)):\n",
    "            train_list[i] = train_list[i].replace('.mp4','')\n",
    "            train_list[i] = train_list[i].replace('.mkv','')\n",
    "            train_list[i] = train_list[i].replace('.webm','')\n",
    "        for i in range(len(test_list)):\n",
    "            test_list[i] = test_list[i].replace('.mp4','')\n",
    "            test_list[i] = test_list[i].replace('.mkv','')\n",
    "            test_list[i] = test_list[i].replace('.webm','')\n",
    "        selected_train_videos=(','.join(train_list))\n",
    "        selected_test_videos=(','.join(test_list))\n",
    "elif current_pipeline == \"MSTCT\":\n",
    "    def on_button_clicked(b):\n",
    "        global selected_videos_string\n",
    "        sel_num_train = int((training_box.value/100) * tot_train_vid)\n",
    "        if sel_num_train < 1:\n",
    "            sel_num_train = 1\n",
    "        sel_num_test = int((testing_box.value/100) * tot_test_vid)\n",
    "        if sel_num_test < 1:\n",
    "            sel_num_test = 1\n",
    "        train_list = training_list[0:sel_num_train]\n",
    "        test_list = testing_list[0:sel_num_test]\n",
    "        combine_dataset = train_list + test_list\n",
    "        selected_videos = list(combine_dataset)\n",
    "        for i in range(len(selected_videos)):\n",
    "            selected_videos[i] = selected_videos[i].replace('.mp4','')\n",
    "        selected_videos_string=(','.join(selected_videos))\n",
    "\n",
    "but.on_click(on_button_clicked)\n",
    "\n",
    "display(but)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model name, Batch size, Epochs\n",
    "After training your model, you will be able to use it in the generation of features on a selected video of your choice. Thus, naming your model is crucial. \n",
    "\n",
    "**Do note that the maximum batch size is 4 and epochs is 1000.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85e9f292dbd4c3cac0e5f353780bf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model name:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5394c93c1f444c0b9c67821da6f3ae81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=2, description='Batch size:', max=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae56804e0654f5a9de1e09e3312299f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=1000, description='Epochs:', max=1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1568ac373f9a4bceb1af655a7a453130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Learning Rate:'), BoundedIntText(value=1)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c234b5dd1742c4aed88a983758be86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Enter', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp = widgets.Text(description='Model name:')\n",
    "batch_size = widgets.BoundedIntText(description='Batch size:',\n",
    "                                   value=2,\n",
    "                                   max=4)\n",
    "epochs = widgets.BoundedIntText(description='Epochs:',\n",
    "                               value=1000,\n",
    "                               max=1000)\n",
    "learning_rate = widgets.BoundedIntText(value=1)\n",
    "button = widgets.Button(description=\"Enter\")\n",
    "\n",
    "button = widgets.Button(description=\"Enter\")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    inp.value\n",
    "    batch_size.value\n",
    "    epochs.value\n",
    "    learning_rate.value\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(inp)\n",
    "display(batch_size)\n",
    "display(epochs)\n",
    "display(widgets.HBox([widgets.Label(value=\"Learning Rate:\"), learning_rate]))\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████▍                                                | 3473/9848 [00:00<00:00, 34717.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB mode ./dataset/\n",
      "load data ./dataset/\n",
      "split!!!! training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 9848/9848 [00:00<00:00, 32496.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 9848/9848 [00:00<00:00, 126119.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split!!!! testing\n",
      "MS_TCT\n",
      "loaded False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiffany\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiffany\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Tiffany\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 2.00 GiB total capacity; 1.03 GiB already allocated; 6.50 MiB free; 1.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\SIT\\S3T1\\3104\\ict3104-p2-team08\\MSTCT\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0mlr_sched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_sched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomp_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\SIT\\S3T1\\3104\\ict3104-p2-team08\\MSTCT\\train.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(models, criterion, num_epochs)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[0mprob_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0msched\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\SIT\\S3T1\\3104\\ict3104-p2-team08\\MSTCT\\train.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, gpu, optimizer, dataloader, epoch)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[0mtrain_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mapm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m                         \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exp_avg'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                         \u001b[1;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                         \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exp_avg_sq'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'amsgrad'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                             \u001b[1;31m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 2.00 GiB total capacity; 1.03 GiB already allocated; 6.50 MiB free; 1.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "global current_pipeline\n",
    "\n",
    "model_name = inp.value\n",
    "epochValue = (epochs.value)\n",
    "batchSize = str(batch_size.value)\n",
    "lrValue = learning_rate.value\n",
    "\n",
    "if current_pipeline == \"TSU\":\n",
    "    %run -i ./Toyota_Smarthome/pipline/train.py -video_train_test={selected_videos_string} -model=PDAN -name={model_name} -batch_size={batchSize} -epoch={epochValue} -lr={lrValue}\n",
    "    %wandb {wandb_username.value}/ICT3104_project\n",
    "elif current_pipeline == \"STEP\":\n",
    "    %run -i ./overwrite_csv_trainval.py --video_train={selected_train_videos} --video_test={selected_test_videos}\n",
    "    %run -i ./scripts/generate_label.py ./datasets/ava/label/new_train.csv\n",
    "    %run -i ./scripts/generate_label.py ./datasets/ava/label/new_val.csv\n",
    "    %run -i ./train.py --model_name={model_name} --batch_size={batchSize} --max_epochs={epochValue} --base_lr={lrValue}\n",
    "    %wandb {wandb_username.value}/ICT3104_project_STEP\n",
    "elif current_pipeline == \"MSTCT\":\n",
    "#     subprocess.call('./run_MSTCT_Charades.sh', shell=True)\n",
    "#     %run ./run_MSTCT_Charades.sh\n",
    "    dataset=\"charades\"\n",
    "    mode='rgb'\n",
    "    model='MS_TCT'\n",
    "    train=True \n",
    "    num_clip='256'\n",
    "    lr='0.0001'\n",
    "    comp_info='False'\n",
    "    epoch='2'\n",
    "    unisize='True'\n",
    "    alpha_l=1.0\n",
    "    beta_l=0.05\n",
    "    batch_size='1'\n",
    "    %run -i ./train.py -dataset={dataset} -mode={mode} -model={model} -train={train} -num_clips={num_clip} -skip=0 -lr={lr} -comp_info={comp_info} -epoch={epoch} -unisize={unisize} -alpha_l=1 -beta_l=0.05 -batch_size={batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4pbFLqFs08xy"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "87233f403a139090a0ef852b9a947df1ef3a4077009893a5c21f86f6450b4b50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
